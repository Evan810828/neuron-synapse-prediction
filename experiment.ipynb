{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "X47A0-clE45g"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzikUgkPr4Mm"
      },
      "source": [
        "## Data Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVmL6hpGriaD",
        "outputId": "628c2cef-cb6a-4215-b2bf-8bb78cec9979"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def dataSetUp():\n",
        "  # load in training data on each potential synapse\n",
        "  data = pd.read_csv(\"./data/train_data.csv\")\n",
        "\n",
        "  # load in additional features for each neuron\n",
        "  feature_weights = pd.read_csv(\"./data/feature_weights.csv\")\n",
        "  morph_embeddings = pd.read_csv(\"./data/morph_embeddings.csv\")\n",
        "\n",
        "  # Merge Data\n",
        "  # join all feature_weight_i columns into a single np.array column\n",
        "  feature_weights[\"feature_weights\"] = (\n",
        "      feature_weights.filter(regex=\"feature_weight_\")\n",
        "      .sort_index(axis=1)\n",
        "      .apply(lambda x: np.array(x), axis=1)\n",
        "  )\n",
        "  # delete the feature_weight_i columns\n",
        "  feature_weights.drop(\n",
        "      feature_weights.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True\n",
        "  )\n",
        "\n",
        "  # join all morph_embed_i columns into a single np.array column\n",
        "  morph_embeddings[\"morph_embeddings\"] = (\n",
        "      morph_embeddings.filter(regex=\"morph_emb_\")\n",
        "      .sort_index(axis=1)\n",
        "      .apply(lambda x: np.array(x), axis=1)\n",
        "  )\n",
        "  # delete the morph_embed_i columns\n",
        "  morph_embeddings.drop(\n",
        "      morph_embeddings.filter(regex=\"morph_emb_\").columns, axis=1, inplace=True\n",
        "  )\n",
        "\n",
        "  data = (\n",
        "      data.merge(\n",
        "          feature_weights.rename(columns=lambda x: \"pre_\" + x),\n",
        "          how=\"left\",\n",
        "          validate=\"m:1\",\n",
        "          copy=False,\n",
        "      )\n",
        "      .merge(\n",
        "          feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
        "          how=\"left\",\n",
        "          validate=\"m:1\",\n",
        "          copy=False,\n",
        "      )\n",
        "      .merge(\n",
        "          morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
        "          how=\"left\",\n",
        "          validate=\"m:1\",\n",
        "          copy=False,\n",
        "      )\n",
        "      .merge(\n",
        "          morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
        "          how=\"left\",\n",
        "          validate=\"m:1\",\n",
        "          copy=False,\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # generate the fw_similarity feature\n",
        "  # cosine similarity function\n",
        "  def row_feature_similarity(row):\n",
        "      pre = row[\"pre_feature_weights\"]\n",
        "      post = row[\"post_feature_weights\"]\n",
        "      return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))\n",
        "\n",
        "      # compute the cosine similarity between the pre- and post- feature weights\n",
        "  data[\"fw_similarity\"] = data.apply(row_feature_similarity, axis=1)\n",
        "\n",
        "  # generate projection group as pre->post\n",
        "  data[\"projection_group\"] = (\n",
        "      data[\"pre_brain_area\"].astype(str)\n",
        "      + \"->\"\n",
        "      + data[\"post_brain_area\"].astype(str)\n",
        "  )\n",
        "\n",
        "  # encoding Non-numerical features\n",
        "  label_encoders = {}\n",
        "  for column in ['compartment', 'pre_brain_area', 'post_brain_area', 'projection_group']:\n",
        "    label_encoders[column] = LabelEncoder()\n",
        "    data[column] = label_encoders[column].fit_transform(data[column])\n",
        "\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv27jpbYzWAj"
      },
      "source": [
        "## Feature Engineeing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MCHeIinOzbF-"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "excluded_feature_indices = [0, 30, 31, 32, 33]\n",
        "\n",
        "def select_features(data, excluded_feature_indices):\n",
        "  data = data.drop(data.columns[excluded_feature_indices], axis=1)\n",
        "  return data\n",
        "\n",
        "def train_test_data_set_up(data):\n",
        "  train_data, test_data = train_test_split(data, test_size=0.2, random_state=1)\n",
        "  # Define the label column name\n",
        "  label_column = 'connected'\n",
        "  train_data_x = train_data.drop(label_column,axis=1)\n",
        "  train_data_y = train_data[label_column]\n",
        "\n",
        "  test_data_x = test_data.drop(label_column, axis=1)\n",
        "  test_data_y = test_data[label_column]\n",
        "\n",
        "  return train_data_x, train_data_y, test_data_x, test_data_y, train_data, test_data\n",
        "\n",
        "def overSampling(data_x, data_y):\n",
        "  # oversample connected neuron pairs\n",
        "  ros = RandomOverSampler(random_state=0)\n",
        "  X_resampled, y_resampled = ros.fit_resample(data_x, data_y)\n",
        "  return X_resampled, y_resampled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "def setup_logging(experiment_name):\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_directory = \"./logging\"  # Replace with your desired path\n",
        "    os.makedirs(log_directory, exist_ok=True)\n",
        "\n",
        "    log_filename = f'{log_directory}/{experiment_name}_{current_time}_metrics_log.log'\n",
        "\n",
        "    logger = logging.getLogger('test')\n",
        "    logger.setLevel(level=logging.INFO)\n",
        "\n",
        "    formatter = logging.Formatter('%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s')\n",
        "\n",
        "    file_handler = logging.FileHandler(log_filename)\n",
        "    file_handler.setLevel(level=logging.INFO)\n",
        "    file_handler.setFormatter(formatter)\n",
        "\n",
        "    # stream_handler = logging.StreamHandler()\n",
        "    # stream_handler.setLevel(logging.INFO)\n",
        "    # stream_handler.setFormatter(formatter)\n",
        "\n",
        "    logger.addHandler(file_handler)\n",
        "    # logger.addHandler(stream_handler)\n",
        "    \n",
        "    return logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK1SsMEG11ij"
      },
      "source": [
        "## Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kTmTaDCa16W5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import balanced_accuracy_score, accuracy_score, confusion_matrix\n",
        "\n",
        "def metric(test_data, logger):\n",
        "    \n",
        "    # compute accuracy\n",
        "    accuracy = accuracy_score(test_data['connected'], test_data['pred'] > .5)\n",
        "    logger.info(f'Accuracy: {accuracy}')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    # confusion matrix\n",
        "    cm = confusion_matrix(test_data['connected'], test_data['pred'] > .5)\n",
        "\n",
        "    # Extracting TN, FP, FN, TP from the confusion matrix\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "    logger.info(f'Confusion Matrix: TN={TN}, FP={FP}, FN={FN}, TP={TP}')\n",
        "    print(f'Confusion Matrix: TN={TN}, FP={FP}, FN={FN}, TP={TP}')\n",
        "\n",
        "    # Calculating Sensitivity (True Positive Rate)\n",
        "    sensitivity = TP / (TP + FN)\n",
        "    logger.info(f'Sensitivity: {sensitivity}')\n",
        "    print(f'Sensitivity: {sensitivity}')\n",
        "\n",
        "    # Calculating Specificity (True Negative Rate)\n",
        "    specificity = TN / (TN + FP)\n",
        "    logger.info(f'Specificity: {specificity}')\n",
        "    print(f'Specificity: {specificity}')\n",
        "\n",
        "    # compute balanced accuracy\n",
        "    balanced_accuracy = balanced_accuracy_score(test_data['connected'], test_data['pred'] > .5)\n",
        "    logger.info(f'Balanced Accuracy: {balanced_accuracy}')\n",
        "    print(balanced_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMKZsB2z4lZe"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xkOU3m5n4omk"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# create pipeline\n",
        "def create_pipe(model, train_data_x, train_data_y, test_data_x, test_data_y, test_data):\n",
        "  pipe = Pipeline(\n",
        "      [(\"scaler\", StandardScaler()), (\"model\", model)]\n",
        "  )\n",
        "  pipe.fit(train_data_x, train_data_y)\n",
        "  test_data[\"pred\"] = pipe.predict_proba(test_data_x)[:,1]\n",
        "\n",
        "  return pipe, test_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOvkALHm6TOZ"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def experiment(experiment_name, method):\n",
        "    data = dataSetUp()\n",
        "    model = method(data)\n",
        "    train_data_x, train_data_y, test_data_x, test_data_y, train_data, test_data = model.data_processing()\n",
        "    pipe, test_data = create_pipe(model.model, train_data_x, train_data_y, test_data_x, test_data_y, test_data)\n",
        "\n",
        "    logger = setup_logging(experiment_name)\n",
        "    metric(test_data, logger=logger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import packages.random_forest as random_forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vYQH_ED6U_7",
        "outputId": "40fc4fa1-8b29-4cae-efe7-ed486315404d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9925740576317701\n",
            "Confusion Matrix: TN=36888, FP=8, FN=268, TP=3\n",
            "Sensitivity: 0.01107011070110701\n",
            "Specificity: 0.9997831743278404\n",
            "0.5054266425144737\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(random_forest)\n",
        "from packages.random_forest import RandomForest\n",
        "experiment_name = \"random_forests\"\n",
        "experiment(experiment_name, RandomForest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import packages.logistic_regression as logistic_regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6925767481905992\n",
            "Confusion Matrix: TN=25515, FP=11381, FN=45, TP=226\n",
            "Sensitivity: 0.8339483394833949\n",
            "Specificity: 0.6915383781439722\n",
            "0.7627433588136836\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(logistic_regression)\n",
        "from packages.logistic_regression import Logistic\n",
        "experiment_name = \"logistic_regression\"\n",
        "experiment(experiment_name, Logistic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNsEzu7DGNdr"
      },
      "source": [
        "## Submission"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
