{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X47A0-clE45g"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzikUgkPr4Mm"
      },
      "source": [
        "## Data Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVmL6hpGriaD",
        "outputId": "628c2cef-cb6a-4215-b2bf-8bb78cec9979"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def dataSetUp():\n",
        "    # load in training data on each potential synapse\n",
        "    data = pd.read_csv(\"./data/train_data.csv\")\n",
        "\n",
        "    # load in additional features for each neuron\n",
        "    feature_weights = pd.read_csv(\"./data/feature_weights.csv\")\n",
        "    morph_embeddings = pd.read_csv(\"./data/morph_embeddings.csv\")\n",
        "\n",
        "    # Merge Data\n",
        "    # join all feature_weight_i columns into a single np.array column\n",
        "    feature_weights[\"feature_weights\"] = (\n",
        "        feature_weights.filter(regex=\"feature_weight_\")\n",
        "        .sort_index(axis=1)\n",
        "        .apply(lambda x: np.array(x), axis=1)\n",
        "    )\n",
        "    # delete the feature_weight_i columns\n",
        "    feature_weights.drop(\n",
        "        feature_weights.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True\n",
        "    )\n",
        "\n",
        "    # join all morph_embed_i columns into a single np.array column\n",
        "    morph_embeddings[\"morph_embeddings\"] = (\n",
        "        morph_embeddings.filter(regex=\"morph_emb_\")\n",
        "        .sort_index(axis=1)\n",
        "        .apply(lambda x: np.array(x), axis=1)\n",
        "    )\n",
        "    # delete the morph_embed_i columns\n",
        "    morph_embeddings.drop(\n",
        "        morph_embeddings.filter(regex=\"morph_emb_\").columns, axis=1, inplace=True\n",
        "    )\n",
        "\n",
        "    data = (\n",
        "        data.merge(\n",
        "            feature_weights.rename(columns=lambda x: \"pre_\" + x),\n",
        "            how=\"left\",\n",
        "            validate=\"m:1\",\n",
        "            copy=False,\n",
        "        )\n",
        "        .merge(\n",
        "            feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
        "            how=\"left\",\n",
        "            validate=\"m:1\",\n",
        "            copy=False,\n",
        "        )\n",
        "        .merge(\n",
        "            morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
        "            how=\"left\",\n",
        "            validate=\"m:1\",\n",
        "            copy=False,\n",
        "        )\n",
        "        .merge(\n",
        "            morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
        "            how=\"left\",\n",
        "            validate=\"m:1\",\n",
        "            copy=False,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # generate the fw_similarity feature\n",
        "    # cosine similarity function\n",
        "    def row_feature_similarity(row):\n",
        "        pre = row[\"pre_feature_weights\"]\n",
        "        post = row[\"post_feature_weights\"]\n",
        "        return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))\n",
        "\n",
        "        # compute the cosine similarity between the pre- and post- feature weights\n",
        "    data[\"fw_similarity\"] = data.apply(row_feature_similarity, axis=1)\n",
        "\n",
        "    # generate projection group as pre->post\n",
        "    data[\"projection_group\"] = (\n",
        "        data[\"pre_brain_area\"].astype(str)\n",
        "        + \"->\"\n",
        "        + data[\"post_brain_area\"].astype(str)\n",
        "    )\n",
        "\n",
        "    # encoding Non-numerical features\n",
        "    label_encoders = {}\n",
        "    for column in ['compartment', 'pre_brain_area', 'post_brain_area', 'projection_group']:\n",
        "        label_encoders[column] = LabelEncoder()\n",
        "        data[column] = label_encoders[column].fit_transform(data[column])\n",
        "\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv27jpbYzWAj"
      },
      "source": [
        "## Feature Engineeing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MCHeIinOzbF-"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "excluded_feature_indices = [0, 30, 31, 32, 33]\n",
        "\n",
        "def select_features(data, excluded_feature_indices):\n",
        "  data = data.drop(data.columns[excluded_feature_indices], axis=1)\n",
        "  return data\n",
        "\n",
        "def train_test_data_set_up(data):\n",
        "  train_data, test_data = train_test_split(data, test_size=0.2, random_state=1)\n",
        "  # Define the label column name\n",
        "  label_column = 'connected'\n",
        "  train_data_x = train_data.drop(label_column,axis=1)\n",
        "  train_data_y = train_data[label_column]\n",
        "\n",
        "  test_data_x = test_data.drop(label_column, axis=1)\n",
        "  test_data_y = test_data[label_column]\n",
        "\n",
        "  return train_data_x, train_data_y, test_data_x, test_data_y, train_data, test_data\n",
        "\n",
        "def overSampling(data_x, data_y):\n",
        "  # oversample connected neuron pairs\n",
        "  ros = RandomOverSampler(random_state=0)\n",
        "  X_resampled, y_resampled = ros.fit_resample(data_x, data_y)\n",
        "  return X_resampled, y_resampled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "def setup_logging(experiment_name):\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_directory = \"./logging\"  # Replace with your desired path\n",
        "    os.makedirs(log_directory, exist_ok=True)\n",
        "\n",
        "    log_filename = f'{log_directory}/{experiment_name}_{current_time}_metrics_log.log'\n",
        "\n",
        "    logger = logging.getLogger('test')\n",
        "    logger.setLevel(level=logging.INFO)\n",
        "\n",
        "    formatter = logging.Formatter('%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s')\n",
        "\n",
        "    file_handler = logging.FileHandler(log_filename)\n",
        "    file_handler.setLevel(level=logging.INFO)\n",
        "    file_handler.setFormatter(formatter)\n",
        "\n",
        "    # stream_handler = logging.StreamHandler()\n",
        "    # stream_handler.setLevel(logging.INFO)\n",
        "    # stream_handler.setFormatter(formatter)\n",
        "\n",
        "    logger.addHandler(file_handler)\n",
        "    # logger.addHandler(stream_handler)\n",
        "    \n",
        "    return logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK1SsMEG11ij"
      },
      "source": [
        "## Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kTmTaDCa16W5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import balanced_accuracy_score, accuracy_score, confusion_matrix\n",
        "\n",
        "def metric(test_data, logger):\n",
        "    \n",
        "    # compute accuracy\n",
        "    accuracy = accuracy_score(test_data['connected'], test_data['pred'] > .5)\n",
        "    logger.info(f'Accuracy: {accuracy}')\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    # confusion matrix\n",
        "    cm = confusion_matrix(test_data['connected'], test_data['pred'] > .5)\n",
        "\n",
        "    # Extracting TN, FP, FN, TP from the confusion matrix\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "    logger.info(f'Confusion Matrix: TN={TN}, FP={FP}, FN={FN}, TP={TP}')\n",
        "    print(f'Confusion Matrix: TN={TN}, FP={FP}, FN={FN}, TP={TP}')\n",
        "\n",
        "    # Calculating Sensitivity (True Positive Rate)\n",
        "    sensitivity = TP / (TP + FN)\n",
        "    logger.info(f'Sensitivity: {sensitivity}')\n",
        "    print(f'Sensitivity: {sensitivity}')\n",
        "\n",
        "    # Calculating Specificity (True Negative Rate)\n",
        "    specificity = TN / (TN + FP)\n",
        "    logger.info(f'Specificity: {specificity}')\n",
        "    print(f'Specificity: {specificity}')\n",
        "\n",
        "    # compute balanced accuracy\n",
        "    balanced_accuracy = balanced_accuracy_score(test_data['connected'], test_data['pred'] > .5)\n",
        "    logger.info(f'Balanced Accuracy: {balanced_accuracy}')\n",
        "    print(f'balanced_accuracy: {balanced_accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMKZsB2z4lZe"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xkOU3m5n4omk"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# create pipeline\n",
        "def create_pipe(model, train_data_x, train_data_y, test_data_x, test_data_y, test_data):\n",
        "  pipe = Pipeline(\n",
        "      [(\"scaler\", StandardScaler()), (\"model\", model)]\n",
        "  )\n",
        "  pipe.fit(train_data_x, train_data_y)\n",
        "  test_data[\"pred\"] = pipe.predict_proba(test_data_x)[:,1]\n",
        "\n",
        "  return pipe, test_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOvkALHm6TOZ"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def experiment(experiment_name, method):\n",
        "    data = dataSetUp()\n",
        "    model = method(data)\n",
        "    train_data_x, train_data_y, test_data_x, test_data_y, train_data, test_data = model.data_processing()\n",
        "    pipe, test_data = create_pipe(model.model, train_data_x, train_data_y, test_data_x, test_data_y, test_data)\n",
        "\n",
        "    logger = setup_logging(experiment_name)\n",
        "    metric(test_data, logger=logger)\n",
        "    \n",
        "    return pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import packages.random_forest as random_forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vYQH_ED6U_7",
        "outputId": "40fc4fa1-8b29-4cae-efe7-ed486315404d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.719751392364194\n",
            "Confusion Matrix: TN=26518, FP=10378, FN=38, TP=233\n",
            "Sensitivity: 0.8597785977859779\n",
            "Specificity: 0.71872289679098\n",
            "0.7892507472884789\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(random_forest)\n",
        "from packages.random_forest import RandomForest\n",
        "experiment_name = \"random_forests\"\n",
        "rf_pipe = experiment(experiment_name, RandomForest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import packages.logistic_regression as logistic_regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6925767481905992\n",
            "Confusion Matrix: TN=25515, FP=11381, FN=45, TP=226\n",
            "Sensitivity: 0.8339483394833949\n",
            "Specificity: 0.6915383781439722\n",
            "0.7627433588136836\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(logistic_regression)\n",
        "from packages.logistic_regression import Logistic\n",
        "experiment_name = \"logistic_regression\"\n",
        "logistic_pipe = experiment(experiment_name, Logistic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import packages.naive_bayes as naive_bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6716980116770254\n",
            "Confusion Matrix: TN=24762, FP=12134, FN=68, TP=203\n",
            "Sensitivity: 0.7490774907749077\n",
            "Specificity: 0.6711296617519514\n",
            "0.7101035762634296\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(naive_bayes)\n",
        "from packages.naive_bayes import NaiveBayes\n",
        "experiment_name = \"naive_bayes\"\n",
        "nb_pipe = experiment(experiment_name, NaiveBayes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import packages.ensemble as ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "importlib.reload(ensemble)\n",
        "from packages.ensemble import Ensemble\n",
        "experiment_name = \"ensemble\"\n",
        "ensemble_pipe = experiment(experiment_name, Ensemble)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import packages.bagging as bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "importlib.reload(bagging)\n",
        "from packages.bagging import Bagging\n",
        "experiment_name = \"bagging\"\n",
        "bagging_pipe = experiment(experiment_name, Bagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNsEzu7DGNdr"
      },
      "source": [
        "## Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def submitData():\n",
        "    lb_data = pd.read_csv(\"data/leaderboard_data.csv\")\n",
        "\n",
        "    # load in additional features for each neuron\n",
        "    feature_weights = pd.read_csv(\"./data/feature_weights.csv\")\n",
        "    morph_embeddings = pd.read_csv(\"./data/morph_embeddings.csv\")\n",
        "\n",
        "    # Merge Data\n",
        "    # join all feature_weight_i columns into a single np.array column\n",
        "    feature_weights[\"feature_weights\"] = (\n",
        "        feature_weights.filter(regex=\"feature_weight_\")\n",
        "        .sort_index(axis=1)\n",
        "        .apply(lambda x: np.array(x), axis=1)\n",
        "    )\n",
        "    # delete the feature_weight_i columns\n",
        "    feature_weights.drop(\n",
        "        feature_weights.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True\n",
        "    )\n",
        "\n",
        "    # join all morph_embed_i columns into a single np.array column\n",
        "    morph_embeddings[\"morph_embeddings\"] = (\n",
        "        morph_embeddings.filter(regex=\"morph_emb_\")\n",
        "        .sort_index(axis=1)\n",
        "        .apply(lambda x: np.array(x), axis=1)\n",
        "    )\n",
        "    # delete the morph_embed_i columns\n",
        "    morph_embeddings.drop(\n",
        "        morph_embeddings.filter(regex=\"morph_emb_\").columns, axis=1, inplace=True\n",
        "    )\n",
        "\n",
        "    # Merge the data\n",
        "    lb_data = (\n",
        "        lb_data.merge(\n",
        "            feature_weights.rename(columns=lambda x: \"pre_\" + x), \n",
        "            how=\"left\", \n",
        "            validate=\"m:1\",\n",
        "            copy=False,\n",
        "        )\n",
        "        .merge(\n",
        "            feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
        "            how=\"left\",\n",
        "            validate=\"m:1\",\n",
        "            copy=False,\n",
        "        )\n",
        "        .merge(\n",
        "            morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
        "            how=\"left\",\n",
        "            validate=\"m:1\",\n",
        "            copy=False,\n",
        "        )\n",
        "        .merge(\n",
        "            morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
        "            how=\"left\",\n",
        "            validate=\"m:1\",\n",
        "            copy=False,\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # generate the fw_similarity feature\n",
        "    # cosine similarity function\n",
        "    def row_feature_similarity(row):\n",
        "        pre = row[\"pre_feature_weights\"]\n",
        "        post = row[\"post_feature_weights\"]\n",
        "        return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))\n",
        "\n",
        "        # compute the cosine similarity between the pre- and post- feature weights\n",
        "    lb_data[\"fw_similarity\"] = lb_data.apply(row_feature_similarity, axis=1)\n",
        "\n",
        "    # generate projection group as pre->post\n",
        "    lb_data[\"projection_group\"] = (\n",
        "        lb_data[\"pre_brain_area\"].astype(str)\n",
        "        + \"->\"\n",
        "        + lb_data[\"post_brain_area\"].astype(str)\n",
        "    )\n",
        "    \n",
        "     # encoding Non-numerical features\n",
        "    label_encoders = {}\n",
        "    for column in ['compartment', 'pre_brain_area', 'post_brain_area', 'projection_group']:\n",
        "        label_encoders[column] = LabelEncoder()\n",
        "        lb_data[column] = label_encoders[column].fit_transform(lb_data[column])\n",
        "    \n",
        "    return lb_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def submit(method, pipe, experiment_name):\n",
        "    lb_data = submitData()\n",
        "    \n",
        "    temp = method([])\n",
        "    li = temp.excluded_feature_indices\n",
        "    li = [i-1 if i > 28 else i for i in li]\n",
        "    \n",
        "    # predict on leaderboard data\n",
        "    predict_X = method([]).select_features(lb_data, li)\n",
        "    lb_data[\"pred\"] = pipe.predict_proba(predict_X)[:, 1]\n",
        "\n",
        "    #create a boolean prediction solution\n",
        "    lb_data[\"connected\"] = lb_data[\"pred\"] > .5\n",
        "    \n",
        "    submission_data = lb_data.filter(['ID','connected'])\n",
        "    #writing csv files\n",
        "    submission_data.to_csv(f'./submission_files/{experiment_name}_submission_data.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 42593 entries, 0 to 42592\n",
            "Data columns (total 35 columns):\n",
            " #   Column                          Non-Null Count  Dtype  \n",
            "---  ------                          --------------  -----  \n",
            " 0   ID                              42593 non-null  int64  \n",
            " 1   axonal_coor_x                   42593 non-null  int64  \n",
            " 2   axonal_coor_y                   42593 non-null  int64  \n",
            " 3   axonal_coor_z                   42593 non-null  int64  \n",
            " 4   dendritic_coor_x                42593 non-null  int64  \n",
            " 5   dendritic_coor_y                42593 non-null  int64  \n",
            " 6   dendritic_coor_z                42593 non-null  int64  \n",
            " 7   adp_dist                        42593 non-null  float64\n",
            " 8   post_skeletal_distance_to_soma  42593 non-null  float64\n",
            " 9   pre_skeletal_distance_to_soma   42593 non-null  float64\n",
            " 10  pre_oracle                      42593 non-null  float64\n",
            " 11  pre_test_score                  42593 non-null  float64\n",
            " 12  pre_rf_x                        42593 non-null  float64\n",
            " 13  pre_rf_y                        42593 non-null  float64\n",
            " 14  post_oracle                     42593 non-null  float64\n",
            " 15  post_test_score                 42593 non-null  float64\n",
            " 16  post_rf_x                       42593 non-null  float64\n",
            " 17  post_rf_y                       42593 non-null  float64\n",
            " 18  compartment                     42593 non-null  int32  \n",
            " 19  pre_brain_area                  42593 non-null  int32  \n",
            " 20  post_brain_area                 42593 non-null  int32  \n",
            " 21  pre_nucleus_x                   42593 non-null  int64  \n",
            " 22  pre_nucleus_y                   42593 non-null  int64  \n",
            " 23  pre_nucleus_z                   42593 non-null  int64  \n",
            " 24  post_nucleus_x                  42593 non-null  int64  \n",
            " 25  post_nucleus_y                  42593 non-null  int64  \n",
            " 26  post_nucleus_z                  42593 non-null  int64  \n",
            " 27  pre_nucleus_id                  42593 non-null  int64  \n",
            " 28  post_nucleus_id                 42593 non-null  int64  \n",
            " 29  pre_feature_weights             42593 non-null  object \n",
            " 30  post_feature_weights            42593 non-null  object \n",
            " 31  pre_morph_embeddings            33275 non-null  object \n",
            " 32  post_morph_embeddings           42593 non-null  object \n",
            " 33  fw_similarity                   42593 non-null  float64\n",
            " 34  projection_group                42593 non-null  int32  \n",
            "dtypes: float64(12), int32(4), int64(15), object(4)\n",
            "memory usage: 11.0+ MB\n"
          ]
        }
      ],
      "source": [
        "submitData().info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 10, 29, 30, 31, 32]\n"
          ]
        }
      ],
      "source": [
        "experiment_name = \"random_forests_2\"\n",
        "submit(RandomForest, rf_pipe, experiment_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
